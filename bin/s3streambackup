#!/usr/bin/env ruby
#
$LOAD_PATH.unshift(File.join(File.dirname(__FILE__), '..', 'lib'))
require 's3streambackup'

S3StreamBackup.new do
	cli do
		stdin :data
		option :key, 
			short: :k,
			description: 'S3 key',
			default: ENV['AWS_ACCESS_KEY_ID']
		option :keep,
			short: :K,
			description: 'how many backup file to keep',
			cast: Integer,
			default: 2
		argument :name,
			description: 'name under which the object will be stored'
	end

	main do |settings, log, s3|
		upload_date = Time.now.utc.strftime "%y%m%d_%H%M%S"
		prefix = "#{settings.prefix}#{settings.name}.backup."
		path = "#{prefix}#{upload_date}#{settings.postfix}"

		bucket = s3.buckets[settings.bucket]
		backup = bucket.objects[path] 
		log.info "writting to: #{path}"

		# make sure we use multipart upload
		total_bytes = 0
		backup.write(
			content_type: 'application/octet-stream',
			estimated_content_length: 10 * 1024 ** 3
		) do |buffer, bytes|
			log.debug "#{total_bytes} bytes written..."
			data = settings.stdin.read(bytes)
			total_bytes += data.bytesize
			buffer.write data
		end
		log.info "total upload size: #{total_bytes}"

		backups = bucket.objects.with_prefix(prefix).to_a

		log.info "keeping maximum #{settings.keep} latest buckups of #{backups.length} storred"

		if backups.length > settings.keep
			backups.take(backups.length - settings.keep).each do |backup|
				log.info "removing oldest backup: #{backup.key}"
				backup.delete
			end
		end
	end
end

